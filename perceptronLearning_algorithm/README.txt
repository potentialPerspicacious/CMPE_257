Generate a data set of size 2000 for training with 1000 +1 examples and 1000 −1 examples.

Tasks:
1. Run the perceptron learning algorithm on the data set above.   
   Reporting the number of updates that the algorithm takes before converging.  
   Plot the examples (Xn,Yn), the target function f, and the final hypothesis g in the same figure.
   
2. Now add some noise to the data set you generated by randomly selecting 50 examples from each class 
   and assigning them wrong output, e.g., assign −1 to examples originally having label +1. 
   
3. Run PLA with pocket algorithm.   
   Plot the new data set from (2), the target function f,
   and the final hypothesis g in the same figure.

Approach:

Create our own target function f and data set D.
and generating a linearly separable data set of size 2000 for training.  
Taking d = 2, and choosing a random line in the plane as our target function f 
where one side of the line maps to +1 and the other maps to −1.  
By choosing the inputs Xn of the data set as random points in the plane, we evaluate the target function 
on each Xn to get the corresponding output Yn. Plotting the examples (Xn,Yn) as well as the target function f on a plane.

***RUN perceptron.py to achive the first task***
***RUN perceptron_noise.py to achive the second task. NOTE: The perceptron algorithm will never converge as there is too much noise, make sure to comment it out before running, but observe the changes in data plot***
***RUN pocket_algo.py to achieve the third task. NOTE: Here you will see the algorithm converging with least mis-classified points***
      

